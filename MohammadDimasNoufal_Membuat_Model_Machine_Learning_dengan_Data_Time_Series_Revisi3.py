# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wMBIWosLEkH14lvbtYK-VySe6h9T3ghc

# Identitas Diri

Nama : Mohammad Dimas Noufal \
Email : dimasnoufal26@gmail.com \
Learning Path : Belajar Pengembangan Machine Learning \
Materi : Proyek Kedua -  Membuat Model Machine Learning dengan Data Time Series \

# Import
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

from keras.models import Sequential
from keras.layers import Dense, LSTM, GRU
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

"""# Dataset"""

data = pd.read_csv('austin_weather.csv')
data.head()

data.isnull().sum()

data.tail()

data.shape

data.info()

dates = data['Date'].values
temp  = data['TempAvgF'].values

"""# Time Series"""

plt.figure(figsize=(15,5))
plt.plot(dates, temp)
plt.title('Temperature average', fontsize=20);

"""# Nilai Training dan Nilai Testing"""

date_latih, date_test, temp_latih, temp_test = train_test_split(dates, temp, test_size=0.2, shuffle=False)

print("Nilai Training : ", len(date_latih))
print("Nilai Testing/Validation : ", len(date_test))

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

"""# Training Model"""

train_set = windowed_dataset(temp_latih, window_size=60, batch_size=100, shuffle_buffer=1000)
test_set = windowed_dataset(temp_test, window_size=60, batch_size=100, shuffle_buffer=1000)

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(30, return_sequences=True,  input_shape=[None, 1]),
  tf.keras.layers.LSTM(30),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

minMae = (data['TempAvgF'].max() - data['TempAvgF'].min()) * 10/100
print("Batas maksimal nilai mae (10%) dari data adalah sebesar", minMae)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if (logs.get('mae')<minMae) & (logs.get('val_mae') < minMae):
      print('MAE sudah mencapai <10%!')
      self.model.stop_training = True
callbacks = myCallback()

optimizer = tf.keras.optimizers.SGD(learning_rate=1.000e-04, momentum=0.9)
model.compile(
    loss=tf.keras.losses.Huber(),
    optimizer=optimizer,
    metrics=['mae']
    )
history = model.fit(
    train_set,
    epochs=200,
    validation_data=test_set,
    verbose=2,
    callbacks=[callbacks],
    )